
# ğŸ“ Lab Assignment No. 4- NLP Preprocessing And Text Classification

## ğŸ“š Course Information

- **Course Name:** Deep Learning  
- **Lab Title:** NLP Techniques for Text Classification  
- **Student Name:** Om Vitole  

---

## ğŸ¯ Objective

The objective of this assignment is to implement NLP preprocessing techniques and build a text classification model using machine learning techniques.

---

## ğŸ¯ Learning Outcomes 
- Understand and apply NLP preprocessing techniques such as tokenization, stopword removal, stemming, and lemmatization.
- Implement text vectorization techniques such as TF-IDF and CountVectorizer.
- Develop a text classification model using a machine learning algorithm.
- Evaluate the performance of the model using suitable metrics.

---

## ğŸ“– Brief Theory

### ğŸ”¹ Natural Language Processing (NLP)

NLP enables machines to understand and interpret human language. It involves:
- **Tokenization** â€“ Splitting text into individual words or tokens.
- **Stopword Removal** â€“ Removing common words that donâ€™t contribute much meaning.
- **Stemming** â€“ Converting words to their root forms.
- **Lemmatization** â€“ More accurate root word extraction using grammar rules.

### ğŸ”¹ Vectorization Techniques

- **CountVectorizer**: Converts text into a matrix of token counts.
- **TF-IDF Vectorizer**: Weighs tokens based on frequency and importance across documents.

### ğŸ”¹ Classification Algorithms

- **Logistic Regression**: A linear model effective for binary/multiclass classification.
- **Naive Bayes (Multinomial)**: A probabilistic classifier well-suited for text data.

---

## ğŸ—‚ Datase
- **Name:** AG News Dataset  
- **Source:**  https://en.innovatiana.com/post/best-datasets-for-text-classification 
- **Classes:** World, Sports, Business, Sci/Tech  
- **Attributes:** Title, Description, Label  

---

## ğŸ§  Models Implemented

### 1. Logistic Regression
- Trained on TF-IDF features.
- Accuracy: **88.62%**
- F1 Score: **0.8862**

### 2. Naive Bayes (Multinomial)
- Trained on CountVectorizer features.
- Accuracy: **88.42%**
- F1 Score: **0.8842**

---

## ğŸ“Š Results Summary

| Model               | Accuracy | F1 Score |
|--------------------|----------|----------|
| Logistic Regression| 88.62%   | 0.8862   |
| Naive Bayes        | 88.42%   | 0.8842   |

---
## ğŸ“ˆ Visualizations

- Confusion Matrix for each model
- Accuracy vs F1 Score Comparison Graph

| Logistic Regression | Naive Bayes |
|---------------------|-------------|
| ![Logistic CM](https://github.com/user-attachments/assets/13c0a41d-98a5-4a3b-ac0c-ef60cfe05a3d) | ![Naive Bayes CM](https://github.com/user-attachments/assets/870688c0-2044-4950-9375-74c580e8f0490) |

| Model Comparison |
|------------------|
| ![Comparison Chart](https://github.com/user-attachments/assets/1c5f738b-7052-4eff-ad8f-fc31315b1da9) |


## ğŸ§ª Evaluation Metrics

- **Accuracy** â€“ Overall correctness.
- **Precision** â€“ True positives out of predicted positives.
- **Recall** â€“ True positives out of actual positives.
- **F1 Score** â€“ Balance of precision and recall.
- **Confusion Matrix** â€“ Visual error analysis tool.

---

## ğŸš€ How to Run

1. Open the notebook in [Google Colab](https://colab.research.google.com/drive/1UhYWeIgNrZ89ciYNcCNjueBwKe1ZvmsB?usp=sharing).
2. Upload the `ag_news` folder containing `train.csv` and `test.csv`.
3. Run each cell sequentially.
4. View classification reports, confusion matrices, and comparison graphs.

---

## ğŸ”— GitHub Repository

ğŸ‘‰ [GitHub Repo Link](https://github.com/meSanika07/DL_PracticalLabAssignmnent_4)

## âš™ï¸ Project Structure

```bash
â”œâ”€â”€ ag_news/
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test.csv
â”œâ”€â”€ NLP_Lab_Assignmnent_4 (1).ipynb
â”œâ”€â”€ README.md
â””â”€â”€ Output Screenshots/
